{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorboardX\n",
    "!pip install --upgrade git+https://github.com/sovrasov/flops-counter.pytorch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "!ln -s \"../gdrive/MyDrive/colab/project-WasteSemSeg/\" \"project-WasteSemSeg\"\n",
    "!ln -s \"../gdrive/MyDrive/colab/dataset/\" \"dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/content/project-WasteSemSeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import copy\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import NLLLoss2d\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.transforms as standard_transforms\n",
    "import torchvision.utils as vutils\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from model import ENet\n",
    "from icnet import ICNet\n",
    "from bisenet_v2 import BiSeNet\n",
    "from config import cfg\n",
    "from loading_data import loading_data\n",
    "from utils import *\n",
    "from timer import Timer\n",
    "from matplotlib import pyplot as plt\n",
    "from loss import *\n",
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "from score import SegmentationMetric\n",
    "import matplotlib.image as mpimg\n",
    "import pdb\n",
    "\n",
    "exp_name = cfg.TRAIN.EXP_NAME\n",
    "device = \"\"\n",
    "log_txt = cfg.TRAIN.EXP_LOG_PATH + '/' + exp_name + '.txt'\n",
    "writer = SummaryWriter(cfg.TRAIN.EXP_PATH+ '/' + exp_name)\n",
    "\n",
    "pil_to_tensor = standard_transforms.ToTensor()\n",
    "train_loader, val_loader, restore_transform = loading_data()\n",
    "metric = SegmentationMetric(cfg.DATA.NUM_CLASSES)\n",
    "class_names = ['none','paper', 'bottle', 'alluminium', 'Nylon']\n",
    "best_results =  {\n",
    "  \"none\": 0.0,\n",
    "  \"paper\": 0.0,\n",
    "  \"bottle\": 0.0,\n",
    "  \"alluminium\": 0.0,\n",
    "  \"Nylon\": 0.0,\n",
    "  \"total\": 0.0,\n",
    "  \"model\":[]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, net, criterion, optimizer, epoch):\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        # inputs = Variable(inputs).cuda()\n",
    "        # labels = Variable(labels).cuda()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        if cfg.MODEL.MODEL == \"enet\" and cfg.DATA.NUM_CLASSES == 1 :\n",
    "            loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "        else:\n",
    "            loss = criterion(outputs, labels)\n",
    "   \n",
    "        # optimizer.zero_grad()\n",
    "        # outputs = net(inputs)\n",
    "        # _, pred_sub4, pred_sub8, pred_sub16 = net(inputs)\n",
    "        # labels = labels.unsqueeze(1).float()\n",
    "        # target_sub4 = F.interpolate(labels, pred_sub4.size()[2:], mode='bilinear', align_corners=True).squeeze(1).long()\n",
    "        # target_sub8 = F.interpolate(labels, pred_sub8.size()[2:], mode='bilinear', align_corners=True).squeeze(1).long()\n",
    "        # target_sub16 = F.interpolate(labels, pred_sub16.size()[2:], mode='bilinear', align_corners=True).squeeze(1).long()\n",
    "        # loss = criterion(pred_sub4, target_sub4)\n",
    "        # loss += criterion(pred_sub8, target_sub8)\n",
    "        # loss += criterion(pred_sub16, target_sub16)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"{i+1}/{len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, net, criterion, optimizer, epoch, restore):\n",
    "    net.eval()\n",
    "    criterion.cpu()\n",
    "    input_batches = []\n",
    "    output_batches = []\n",
    "    label_batches = []\n",
    "    iou_ = 0.0\n",
    "    \n",
    "    iou_sum_classes = [0.0] * cfg.DATA.NUM_CLASSES\n",
    "    metric.reset()\n",
    "    for vi, data in enumerate(val_loader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "       \n",
    "        # inputs = Variable(inputs, volatile=True).cuda()\n",
    "        # labels = Variable(labels, volatile=True).cuda()\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        if not cfg.MODEL.MODEL == \"enet\":\n",
    "            outputs = outputs[0]\n",
    "        \n",
    "        #metric.update(outputs, labels)\n",
    "        #outputs,_,_,_ = net(inputs)\n",
    "        #for binary classification\n",
    "        if cfg.DATA.NUM_CLASSES == 1:\n",
    "            outputs[outputs>0.5] = 1\n",
    "            outputs[outputs<=0.5] = 0\n",
    "            x , _ = calculate_mean_iu(outputs.squeeze_(1).data.cpu().numpy(), labels.data.cpu().numpy(), 2)\n",
    "            iou_ += x\n",
    "        else:\n",
    "            x , y = calculate_mean_iu(outputs.argmax(dim=1).data.cpu().numpy(), labels.data.cpu().numpy(), cfg.DATA.NUM_CLASSES)\n",
    "            iou_ += x\n",
    "            iou_sum_classes = [sum(x) for x in zip(iou_sum_classes, y)]\n",
    "\n",
    "            \n",
    "             \n",
    "            \n",
    "            # for c in range(cfg.DATA.NUM_CLASSES):\n",
    "            # # predmask\n",
    "            #     pred_mask = (outputs.argmax(dim=1) == c).cpu().numpy()\n",
    "            #     labels_mask = (labels == c).cpu().numpy()\n",
    "            #     class_iou = calculate_mean_iu(pred_mask, labels_mask, 2)\n",
    "            #     iou_sum_classes[c] += class_iou\n",
    "\n",
    "\n",
    "\n",
    "    #IoU,mIoU = metric.get()\n",
    "    if cfg.DATA.NUM_CLASSES == 1:\n",
    "        \n",
    "        print('[mean iu %.4f]' % (iou_ / len(val_loader)))\n",
    "\n",
    "        if iou_/len(val_loader) > best_results[\"total\"]:\n",
    "            \n",
    "            best_results[\"total\"] = iou_/len(val_loader)\n",
    "            best_results[\"model\"] = net\n",
    "\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        #mean_iu_classes = [x for x in IoU]\n",
    "        mean_iu_classes = [x / len(val_loader) for x in iou_sum_classes]\n",
    "        # Print the mean IoU for each class\n",
    "        class_names = ['none','paper', 'bottle', 'alluminium', 'Nylon']\n",
    "        for i, class_name in enumerate(class_names):\n",
    "            #print(f'Mean IoU for {class_name}: {IoU[i]}')\n",
    "            print(f'Mean IoU for {class_name}: {mean_iu_classes[i]:.6f}')\n",
    "        print('[mean iu %.4f]' % (iou_/len(val_loader) ))\n",
    "    \n",
    "        if iou_/len(val_loader) > best_results[\"total\"]:\n",
    "            best_iou = iou_/len(val_loader)\n",
    "        \n",
    "            for i,v in enumerate(list(best_results.keys())[:-2]):\n",
    "                best_results[v] = mean_iu_classes[i]\n",
    "\n",
    "            best_results[\"total\"] = iou_/len(val_loader)\n",
    "            best_results[\"model\"] = net\n",
    "\n",
    "            # flops, params = get_model_complexity_info(net, (3, 224, 448), as_strings=True,\n",
    "            #                             print_per_layer_stat=False, verbose=False)\n",
    "   \n",
    "    net.train()\n",
    "    criterion.cuda()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(dsize):\n",
    "\n",
    "    total_flops, _ = get_model_complexity_info(best_results['model'], (3, 224, 448), as_strings=True,\n",
    "                                        print_per_layer_stat=False, verbose=False,flops_units=\"GMac\")\n",
    "   \n",
    "    \n",
    "    param_size = 0\n",
    "    for param in best_results['model'].parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in best_results['model'].buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "\n",
    "    print(\"Best Model \")\n",
    "    if cfg.DATA.NUM_CLASSES > 1:\n",
    "        for v in list(best_results.keys())[:-1]:\n",
    "            print(f'Mean IoU for {v}: {best_results[v]:.6f}')\n",
    "    \n",
    "    print('[mean iu %.4f]' % (best_results['total']))\n",
    "    print(\"Performance {} GFlops\".format(float(total_flops.split(\" \")[0]) * 2))\n",
    "    print(\"Number Parameters {}\".format(param_size))\n",
    "    print(\"Size Model {} MB\".format((param_size + buffer_size) / 1024**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.TRAIN.CUDA and torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "cfg_file = open('./config.py',\"r\")  \n",
    "cfg_lines = cfg_file.readlines()\n",
    "\n",
    "with open(log_txt, 'a') as f:\n",
    "        f.write(''.join(cfg_lines) + '\\n\\n\\n\\n')\n",
    "# if len(cfg.TRAIN.GPU_ID)==1:\n",
    "#     torch.cuda.set_device(cfg.TRAIN.GPU_ID[0])\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "\n",
    "net = []   \n",
    "#net = ICNet(1)\n",
    "#et = ENet(only_encode=True)\n",
    "#net = net.to(\"cpu\")\n",
    "if cfg.TRAIN.STAGE=='all':\n",
    "    if cfg.MODEL.MODEL == \"icnet\":\n",
    "        net = ICNet(cfg.DATA.NUM_CLASSES)\n",
    "    elif cfg.MODEL.MODEL == \"enet\":    \n",
    "        net = ENet(only_encode=False)\n",
    "    elif cfg.MODEL.MODEL == \"bisenet\":\n",
    "        net = BiSeNet(cfg.DATA.NUM_CLASSES,True)\n",
    "    if cfg.TRAIN.PRETRAINED_ENCODER != '':\n",
    "        encoder_weight = torch.load(cfg.TRAIN.PRETRAINED_ENCODER)\n",
    "        del encoder_weight['classifier.bias']\n",
    "        del encoder_weight['classifier.weight']\n",
    "        # pdb.set_trace()\n",
    "        net.encoder.load_state_dict(encoder_weight)\n",
    "elif cfg.TRAIN.STAGE =='encoder':\n",
    "    net = ENet(only_encode=True)\n",
    "\n",
    "\n",
    "if len(cfg.TRAIN.GPU_ID)>1:\n",
    "    net = torch.nn.DataParallel(net, device_ids=cfg.TRAIN.GPU_ID).cuda()\n",
    "else:\n",
    "    net=net.to(device)\n",
    "\n",
    "net.train()\n",
    "\n",
    "if cfg.MODEL.MODEL == \"enet\":\n",
    "    criterion =  torch.nn.CrossEntropyLoss().to(device) if cfg.DATA.NUM_CLASSES > 1 else torch.nn.BCEWithLogitsLoss().to(device)\n",
    "elif cfg.MODEL.MODEL == \"icnet\":\n",
    "    criterion =  Multiclass_ICNetLoss().to(device)  if cfg.DATA.NUM_CLASSES > 1 else Binary_ICNetLoss().to(device)\n",
    "elif cfg.MODEL.MODEL == \"bisenet\":\n",
    "    criterion = MixSoftmaxCrossEntropyLoss(True).to(device)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=cfg.TRAIN.LR, weight_decay=cfg.TRAIN.WEIGHT_DECAY)\n",
    "scheduler = StepLR(optimizer, step_size=cfg.TRAIN.NUM_EPOCH_LR_DECAY, gamma=cfg.TRAIN.LR_DECAY)\n",
    "_t = {'train time' : Timer(),'val time' : Timer()} \n",
    "validate(val_loader, net, criterion, optimizer, -1, restore_transform)\n",
    "for epoch in range(cfg.TRAIN.MAX_EPOCH):\n",
    "    _t['train time'].tic()\n",
    "    train(train_loader, net, criterion, optimizer, epoch)\n",
    "    _t['train time'].toc(average=False)\n",
    "    print('training time of one epoch: {:.2f}s'.format(_t['train time'].diff))\n",
    "    _t['val time'].tic()\n",
    "    validate(val_loader, net, criterion, optimizer, epoch, restore_transform)\n",
    "    _t['val time'].toc(average=False)\n",
    "    print('val time of  epoch {}: {:.2f}s'.format(epoch,_t['val time'].diff))\n",
    "\n",
    "benchmark(next(iter(val_loader))[0].shape)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
